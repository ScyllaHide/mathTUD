\section{Wartezeitverteilungen}

\subsection{Negative Binomialverteilung}

Wir wiederholen ein Bernoulliexperiment mit Erfolgswahrscheinlichkeit $p \in [0,1]$ unendlich oft. Gesucht ist die Anzahl der Misserfolge bis zum $r$-ten Erfolg ($r \in \N$). Ein passender Ergebnisraum ist $\Omega = \N_0$. Für Modellierung ist es jedoch leichter in jedem Versuch erfolgt (``1'') oder Misserfolg (``0'') festzuhalten und mit dem unendlichen Produktmaß des Bernoullimaßes auf $\menge{0,1}^{\N}$ zu arbeiten. 

Als Zufallsvariable wählen wir
\begin{equation*}
	\abb{X_r}{\menge{0,1}^\N}{\Omega}
\end{equation*}
welche die Anzahl der Misserfolge bis zum $r$-ten Erfolg darstellt. Setze
\begin{equation*}
	X_r(\omega) = \min\menge{k : \sum_{i=1}^{k} \omega_i = r} - r
\end{equation*}
Dann ist
\begin{equation}
	\P(X_r = k) = \sum_{\omega \in \menge{0,1}^\N, X_r(\omega) = k} \prod_{i=1}^{\infty} \rho(\omega_i) \tag{$\star$} \label{eq_negbin1}
\end{equation}
mit $\rho(0) = 1-p$ und $\rho(1) = p$ (Zähldichte der Binomialverteilung), also
\begin{equation*}
	\P(X_r = k) = \binom{r+k-1}{k} (1-p)^k p^r \quad (k \in \N)
\end{equation*}

\begin{*anmerkung}
	Alternativ kann \cref{eq_negbin1} auch durch
	\begin{equation*}
		\P(X_r = k) = \sum_{\omega \in \menge{0,1}^{r+k}, X_r(\omega) = k} \prod_{i=1}^{r+k} \rho(\omega_i)
	\end{equation*}
	ersetzt werden.
\end{*anmerkung}

\begin{definition}[negative Binomialverteilung]
	\label{4_1_defintion}
	Sei $p \in[0,1]$ und $r \in \N$, dann heißt die Verteilung auf $\N_0$ mit Zähldichte
	\begin{equation*}
		\rho(k) = \binom{r+k-1}{k} (1-p)^k p^r 
	\end{equation*}
	die \begriff{negative Binomialverteilung} mit Parametern $(r,p)$. Schreibe $\negBin(r,p)$. Im Fall $r = 1$ nennen wir die Verteilung mit Zähldichte
	\begin{equation*}
		\rho(k) = p (1-p)^k \quad (k \in \N_0)
	\end{equation*}
	\begriff{geometrische Verteilung} mit Parameter $p$. Schreibe $\Geom(p)$.
\end{definition}


\subsection{Exponential- und Gammaverteilung}

\begin{description}
	\item[Ziel:] Modelliere die Wartezeit auf $r$ Ereignisse in kontinuierlicher Zeit
	\item[Wähle:] $(\Omega, \F) = (\R_+, \borel(\R_+))$
	\item[Annahmen:] ~
	\begin{itemize}
		\item Jedes Ereignis geschieht zu einer zufälligen Zeit.
		\item Die Anzahl der Ereignisse bis zur Zeit $t$ sei $\Poisson(\lambda t)$ verteilt.
	\end{itemize}
\end{description}

Die zweite Annahme macht Sinn, denn
\begin{itemize}
	\item Poissonverteilung ist Modell für Anzahl seltener Ereignisse
	\item Nach Beispiel 3.26 gilt $\Poisson(\lambda t) \ast \Poisson(\lambda s) = \Poisson(\lambda(t+s))$. 
	Die Linearität des Parameters entspricht also einer Stationaritätsvorrausetzung -- Modelliert $X \sim \Poisson(\lambda t)$ die Ereignisse in $(0,t]$ und $Y \sim \Poisson(\lambda s)$ die Ereignisse in $(t,t+s]$, so modelliert $X+Y \sim \Poisson(\lambda (t+s))$ die Ereignisse in $(0,t+s]$.
\end{itemize}

Unter diesen Annahmen folgt für die Wahrscheinlichkeit in $(0,t]$ mindestens $r$ Ereignisse zu beobachten
\begin{equation*}
	\P ((0,t)) = 1-\sum_{k=0}^{r-1} \underbrace{e^{-\lambda t} \frac{(\lambda t)^k}{k!}}_{\text{Zähldichte } \Poisson(\lambda t)\text{ in } k}
\end{equation*}
Wegen
\begin{equation*}
\begin{aligned}
	\ableitung{t} \brackets{1- \sum_{k=0}^{r-1} e^{-\lambda t} \frac{(\lambda t)^k}{k!}}
	&= - (-\lambda) e^{-\lambda t} \sum_{k=0}^{r-1} \frac{(\lambda t)^k}{k!} - e^{-\lambda t} \sum_{k=1}^{r-1} \frac{\lambda^k t^{k-1}}{(k-1)!}  \\
	&= e^{-\lambda t} \left( \sum_{k=0}^{r-1} \frac{\lambda^{k+1} t^k}{k!} - \sum_{k=0}^{r-2} \frac{\lambda^{k+1} t^k}{k!} \right) \\
	&= e^{-\lambda t} \frac{\lambda^r t^{r-1}}{(r-1)!} 
\end{aligned}
\end{equation*}
gilt
\begin{equation*}
	\P((0,t)) = \int_{0}^t e^{-\lambda x} \frac{\lambda^r x^{r-1}}{(r-1)!} \dx
\end{equation*}

Wir definieren allgemeiner:
\begin{definition}[Gammaverteilung, Gammafunktion]
	\label{4_2_definition}
	Seien $\lambda > 0$ und $r>0$. Dann heißt die Verteilung auf $(\R_+, \borel{\R_+})$ mit Dichte
	\begin{equation*}
		f(x) = \frac{\lambda^r}{\Gamma(r)} x^{r-1} e^{-\lambda x} \quad \mit \quad 
		\Gamma(r) = \int_{0}^{\infty} y^{r-1}e^{-y} \dy \quad (r>0)
	\end{equation*}
	\begriff{Gammaverteilung} mit Parametern $\lambda,r$. Schreibe $\GammaDist(\lambda,r)$.
\end{definition}

Insbesondere ist $\GammaDist(\lambda,1)$ gerade die \begriff{Exponentialverteilung} $\Exp(\lambda)$ (vgl. \cref{beispiel: 1_1.7_exponentialverteilung}).

\begin{*anmerkung}
	Die Funktion $\Gamma$ in \cref{4_2_definition} wird auch als Gamma-Funktion bezeichnet.
\end{*anmerkung}

Die Gammaverteilung ist \begriff{reproduktiv}: Die Wartezeit auf $r+s$ Ereignisse entspricht der Wartezeit auf $r$ Ereignisse + $s$ (weitere) Ereignisse:
\begin{lemma}
	\label{4_3_lemma}
	Seien $X \sim \GammaDist(\lambda,r), Y \sim \GammaDist(\lambda,s)$ unabhängig. Dann gilt 
	\begin{equation*}
		X+Y \sim \GammaDist(\lambda,r+s)
	\end{equation*}
\end{lemma}
\begin{proof}
	Hier nur für $r,s \in \N$, allgemein später mit momenterzeugenden Funktionen. 
	Seien $\rho_X$, $\rho_Y$ Dichten von $X$,$Y$. Nach \cref{3_25_satz} folgt
\begin{align*}
	\rho_{X+Y}(x) 
	&= (\rho_X \ast \rho_Y) (x) 
	= \int_{\R} \rho_X(y)\rho_Y(x-y) \dy  \\
	&= \int_0^x \frac{\lambda^r}{\Gamma(r)} y^{r-1} e^{-\lambda y} \frac{\lambda^s}{\Gamma(s)} (x-y)^{s-1} e^{-\lambda (x-y)} \dy  \\
	&= \frac{\lambda^{r+s}}{\Gamma(r)\Gamma(s)} e^{-\lambda x} \int_0^x y^{r-1} (x-y)^{s-1} \dy  \\
	\overset{\text{P.I.}}&{=} \frac{\lambda^{r+s}}{\Gamma(r)\Gamma(s)} e^{-\lambda x} \brackets{\underbrace{\left[\frac{1}{r} y^r (x-y)^{s-1}\right]_{y=0}^x}_{=0} + \frac{s-1}{r} \int_0^x y^r (x-y)^{s-2} \dy }  \\
	\overset{\text{Induktion}}&{=} \frac{\lambda^{r+s}}{\Gamma(r)\Gamma(r)} e^{-\lambda x} \frac{\Gamma(r)\Gamma(s)}{\Gamma(r+s)} = \frac{\lambda^{r+s}}{\Gamma(r+s)} e^{-\lambda x}
\end{align*}
\end{proof}

Exponentialverteilungen sind zudem \begriff{gedächtnislos}:
\begin{lemma}[Gedächtnislosigkeit der Exponentialverteilung]
	\label{4_4_lemma}
	Sei $X \sim \Exp(\lambda)$. Dann gilt
	\begin{equation*}
		\P(X>t) = \P(X > t+s \mid X > s) \quad (t,s \ge 0) \tag{$\star$}\label{4_4_eq:gedaechtnislos}
	\end{equation*}
\end{lemma}
\begin{proof}
	\begin{align*}
		\P(X > t+s \mid X \ge s) &= \frac{\P(X > t+s, X > s)}{\P(X > s)} \\
		&= \frac{\P(X > t+s)}{\P(X > s)} \\
		&= \frac{e^{-\lambda(t+s)}}{e^{-\lambda t}} = e^{-\lambda t} = \P(X> t)
	\end{align*}
\end{proof}

\begin{beispiel}
	\label{4_5_beispiel}
	Eine Studentin wartet morgens eine $\Exp(\sfrac{1}{5})$ verteilte Zeit $X$ auf den Bus zur Uni. Die Wahrscheinlichkeit einer Wartezeit $\ge 5$ Minuten beträgt
	\begin{equation*}
		\P(X \ge 5) = e^{-\frac{1}{5} * 5} = e^{-1} \approx 0,37
	\end{equation*}
	An einem kalten, stürmischen Frühlingstag hat die Studentin bereits 10~Minuten gewartet. Die Wahrscheinlichkeit mindestens fünf weitere Minuten zu warten ist
	\begin{equation*}
		\P(X\ge 15 \mid X \ge 10) = \P(X \ge 5) = e^{-1} \approx 0,37
	\end{equation*}
\end{beispiel}

\begin{*bemerkung}
	Man kann sogar zeigen, dass die Exponentialverteilung die einzige absolutstetige Verteilung mit der Eigenschaft \labelcref{4_4_eq:gedaechtnislos} ist.
\end{*bemerkung}